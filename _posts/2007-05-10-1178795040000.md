---
title: ThesaurusAnalyzer
author: jolestar
excerpt: '<p>一个基于词库的中文分词程序</p>'
layout: post
permalink:  /1178795040000/
tags:
  - 分词
  - 全部
  - java
  - lucene
  - 搜索引擎
---
# 

前一段时间用lucene做一个搜索程序,找了好长时间的中文分词程序,都没找到合适的,最后自己弄了一个.现在共享出来.希望对大家有用.  
      分词算法:    基于词库的正向最大匹配算法.  
      分词词库用的是网上一个叫 segmenter 的分词程序使用的词库.  
      地址:[www.mandarintools.com/segmenter.html][1]   

 [1]: http://www.mandarintools.com/segmenter.html

       
这个segmenter分词程序是把文件按行读取出来,然后把一行假设为一个词,从库中匹配,如果匹配不成功,则去掉一个字,再继续匹配.这样的分词程  
序,其一,不便在lucene中使用,因为lucene的analyzer是通过Tokenizer分词的,而Tokenizer中一般是对字符流进行处  
理,每次next返回一个Token,并不是一次性把内容读取进来,处理后再返回结果.其二,按行读取会有个缺点,就是如果文本中恰好把一个词用换行符隔  
开了,这样这个词也就被切开了,没有被当作一个词处理.  
      
     我的程序实现方式:把词库读进内存后构建一个词语树.树的每个节点包含一个字. 比方  中国   中国人  中华民族  中华人民共和国  几个词,构成的树的结构:  
     
                 中  
           国^    华  
      人^        人    民  
                   民       族^  
                   共  
                   和  
                   国^

        
懒得上传图片,所以将就着这样表示了.^表示该节点可以构成一个词.分词的过程类似于输入法的联想功能.读取一个字,然后联想,直到联想到不能为止.如果  
当前可以构成词,便返回一个Token.如果当前不能构成词语,便回溯到最近的可以构成词语的节点,返回.最差的情况就是返回第一个单字.然后从返回结果  
的下一个字重新开始联想.

      lucene自带的几个分词程序中,ChineseAnalyzer是按字分的,与StandardAnalyzer对中文的分词没有大的区别.CJKAnalyzer是按两字切分的,比较武断,并且会产生垃圾Token,影响索引大小.   
    
     本分词程序的效果取决与词库.您可以用自己的词库替换程序自带的词库.词库是一个文本文件,名称为word.txt. 每一行一个词语,以#开头表示跳过改行.最后保存为UTF-8的文本.

    程序的缺陷:  
        没有加入识别人名和地名的功能
